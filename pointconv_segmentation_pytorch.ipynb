{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from open3d import *\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 21\n",
    "block_points = 8091\n",
    "root = '/media/ken/B60A03C60A03829B/data/scannet'\n",
    "BATCH_SIZE = 8\n",
    "NUM_POINT = 8091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_data(batch_data):\n",
    "    \"\"\" Normalize the batch data, use coordinates of the block centered at origin,\n",
    "        Input:\n",
    "            BxNxC array\n",
    "        Output:\n",
    "            BxNxC array\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    normal_data = np.zeros((B, N, C))\n",
    "    for b in range(B):\n",
    "        pc = batch_data[b]\n",
    "        centroid = np.mean(pc, axis=0)\n",
    "        pc = pc - centroid\n",
    "        m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n",
    "        pc = pc / m\n",
    "        normal_data[b] = pc\n",
    "    return normal_data\n",
    "\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    \"\"\" Shuffle data and labels.\n",
    "        Input:\n",
    "          data: B,N,... numpy array\n",
    "          label: B,... numpy array\n",
    "        Return:\n",
    "          shuffled data, label and shuffle indices\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(labels))\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx, ...], labels[idx], idx\n",
    "\n",
    "def shuffle_points(batch_data):\n",
    "    \"\"\" Shuffle orders of points in each point cloud -- changes FPS behavior.\n",
    "        Use the same shuffling idx for the entire batch.\n",
    "        Input:\n",
    "            BxNxC array\n",
    "        Output:\n",
    "            BxNxC array\n",
    "    \"\"\"\n",
    "    idx = np.arange(batch_data.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    return batch_data[:,idx,:]\n",
    "\n",
    "def rotate_point_cloud(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "def rotate_point_cloud_z(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, sinval, 0],\n",
    "                                    [-sinval, cosval, 0],\n",
    "                                    [0, 0, 1]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "def rotate_point_cloud_with_normal(batch_xyz_normal):\n",
    "    ''' Randomly rotate XYZ, normal point cloud.\n",
    "        Input:\n",
    "            batch_xyz_normal: B,N,6, first three channels are XYZ, last 3 all normal\n",
    "        Output:\n",
    "            B,N,6, rotated XYZ, normal point cloud\n",
    "    '''\n",
    "    for k in range(batch_xyz_normal.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_xyz_normal[k,:,0:3]\n",
    "        shape_normal = batch_xyz_normal[k,:,3:6]\n",
    "        batch_xyz_normal[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "        batch_xyz_normal[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), rotation_matrix)\n",
    "    return batch_xyz_normal\n",
    "\n",
    "def rotate_perturbation_point_cloud_with_normal(batch_data, angle_sigma=0.06, angle_clip=0.18):\n",
    "    \"\"\" Randomly perturb the point clouds by small rotations\n",
    "        Input:\n",
    "          BxNx6 array, original batch of point clouds and point normals\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n",
    "        Rx = np.array([[1,0,0],\n",
    "                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n",
    "                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n",
    "        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n",
    "                       [0,1,0],\n",
    "                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n",
    "        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n",
    "                       [np.sin(angles[2]),np.cos(angles[2]),0],\n",
    "                       [0,0,1]])\n",
    "        R = np.dot(Rz, np.dot(Ry,Rx))\n",
    "        shape_pc = batch_data[k,:,0:3]\n",
    "        shape_normal = batch_data[k,:,3:6]\n",
    "        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), R)\n",
    "        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), R)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def rotate_point_cloud_by_angle(batch_data, rotation_angle):\n",
    "    \"\"\" Rotate the point cloud along up direction with certain angle.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        #rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k,:,0:3]\n",
    "        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "def rotate_point_cloud_by_angle_with_normal(batch_data, rotation_angle):\n",
    "    \"\"\" Rotate the point cloud along up direction with certain angle.\n",
    "        Input:\n",
    "          BxNx6 array, original batch of point clouds with normal\n",
    "          scalar, angle of rotation\n",
    "        Return:\n",
    "          BxNx6 array, rotated batch of point clouds iwth normal\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        #rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k,:,0:3]\n",
    "        shape_normal = batch_data[k,:,3:6]\n",
    "        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1,3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "\n",
    "def rotate_perturbation_point_cloud(batch_data, angle_sigma=0.06, angle_clip=0.18):\n",
    "    \"\"\" Randomly perturb the point clouds by small rotations\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n",
    "        Rx = np.array([[1,0,0],\n",
    "                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n",
    "                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n",
    "        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n",
    "                       [0,1,0],\n",
    "                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n",
    "        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n",
    "                       [np.sin(angles[2]),np.cos(angles[2]),0],\n",
    "                       [0,0,1]])\n",
    "        R = np.dot(Rz, np.dot(Ry,Rx))\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, jittered batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert(clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data\n",
    "\n",
    "def shift_point_cloud(batch_data, shift_range=0.1):\n",
    "    \"\"\" Randomly shift point cloud. Shift is per point cloud.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, shifted batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    shifts = np.random.uniform(-shift_range, shift_range, (B,3))\n",
    "    for batch_index in range(B):\n",
    "        batch_data[batch_index,:,:] += shifts[batch_index,:]\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):\n",
    "    \"\"\" Randomly scale the point cloud. Scale is per point cloud.\n",
    "        Input:\n",
    "            BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "            BxNx3 array, scaled batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    scales = np.random.uniform(scale_low, scale_high, B)\n",
    "    for batch_index in range(B):\n",
    "        batch_data[batch_index,:,:] *= scales[batch_index]\n",
    "    return batch_data\n",
    "\n",
    "def random_point_dropout(batch_pc, max_dropout_ratio=0.875):\n",
    "    ''' batch_pc: BxNx3 '''\n",
    "    for b in range(batch_pc.shape[0]):\n",
    "        dropout_ratio =  np.random.random()*max_dropout_ratio # 0~0.875\n",
    "        drop_idx = np.where(np.random.random((batch_pc.shape[1]))<=dropout_ratio)[0]\n",
    "        if len(drop_idx)>0:\n",
    "            batch_pc[b,drop_idx,:] = batch_pc[b,0,:] # set to the first point\n",
    "    return batch_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scannet dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannetDataset():\n",
    "    def __init__(self, root, block_points=2048, split='train', with_rgb = True):\n",
    "        self.npoints = block_points\n",
    "        self.root = root\n",
    "        self.with_rgb = with_rgb\n",
    "        self.split = split\n",
    "        self.data_filename = os.path.join(self.root, 'scannet_%s_rgb21c_pointid.pickle'%(split))\n",
    "        with open(self.data_filename,'rb') as fp:\n",
    "            self.scene_points_list = pickle.load(fp)\n",
    "            self.semantic_labels_list = pickle.load(fp)\n",
    "            self.scene_points_id = pickle.load(fp)\n",
    "            self.scene_points_num = pickle.load(fp)\n",
    "        if split=='train':\n",
    "            labelweights = np.zeros(21)\n",
    "            for seg in self.semantic_labels_list:\n",
    "                tmp,_ = np.histogram(seg,range(22))\n",
    "                labelweights += tmp\n",
    "            labelweights = labelweights.astype(np.float32)\n",
    "            labelweights = labelweights/np.sum(labelweights)\n",
    "            self.labelweights = np.power(np.amax(labelweights[1:]) / labelweights, 1/3.0)\n",
    "            print(self.labelweights)\n",
    "        elif split=='val':\n",
    "            self.labelweights = np.ones(21)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.with_rgb:\n",
    "            point_set = self.scene_points_list[index]\n",
    "        else:\n",
    "            point_set = self.scene_points_list[index][:, 0:3]\n",
    "        semantic_seg = self.semantic_labels_list[index].astype(np.int32)\n",
    "        coordmax = np.max(point_set[:, 0:3],axis=0)\n",
    "        coordmin = np.min(point_set[:, 0:3],axis=0)\n",
    "        isvalid = False\n",
    "        for i in range(10):\n",
    "            curcenter = point_set[np.random.choice(len(semantic_seg),1)[0],0:3]\n",
    "            curmin = curcenter-[0.75,0.75,1.5]\n",
    "            curmax = curcenter+[0.75,0.75,1.5]\n",
    "            curmin[2] = coordmin[2]\n",
    "            curmax[2] = coordmax[2]\n",
    "            curchoice = np.sum((point_set[:, 0:3]>=(curmin-0.2))*(point_set[:, 0:3]<=(curmax+0.2)),axis=1)==3\n",
    "            cur_point_set = point_set[curchoice,0:3]\n",
    "            cur_point_full = point_set[curchoice,:]\n",
    "            cur_semantic_seg = semantic_seg[curchoice]\n",
    "            if len(cur_semantic_seg)==0:\n",
    "                continue\n",
    "            mask = np.sum((cur_point_set>=(curmin-0.01))*(cur_point_set<=(curmax+0.01)),axis=1)==3\n",
    "            vidx = np.ceil((cur_point_set[mask,:]-curmin)/(curmax-curmin)*[31.0,31.0,62.0])\n",
    "            vidx = np.unique(vidx[:,0]*31.0*62.0+vidx[:,1]*62.0+vidx[:,2])\n",
    "            isvalid = np.sum(cur_semantic_seg>0)/len(cur_semantic_seg)>=0.7 and len(vidx)/31.0/31.0/62.0>=0.02\n",
    "            if isvalid:\n",
    "                break\n",
    "        choice = np.random.choice(len(cur_semantic_seg), self.npoints, replace=True)\n",
    "        point_set = cur_point_full[choice,:]\n",
    "        semantic_seg = cur_semantic_seg[choice]\n",
    "        mask = mask[choice]\n",
    "        sample_weight = self.labelweights[semantic_seg]\n",
    "        sample_weight *= mask\n",
    "        return point_set, semantic_seg, sample_weight\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.scene_points_list)\n",
    "\n",
    "class ScannetDatasetWholeScene():\n",
    "    def __init__(self, root, block_points=8192, split='val', with_rgb = True):\n",
    "        self.npoints = block_points\n",
    "        self.root = root\n",
    "        self.with_rgb = with_rgb\n",
    "        self.split = split\n",
    "        self.data_filename = os.path.join(self.root, 'scannet_%s_rgb21c_pointid.pickle'%(split))\n",
    "        with open(self.data_filename,'rb') as fp:\n",
    "            self.scene_points_list = pickle.load(fp)\n",
    "            self.semantic_labels_list = pickle.load(fp)\n",
    "            self.scene_points_id = pickle.load(fp)\n",
    "            self.scene_points_num = pickle.load(fp)\n",
    "        if split=='train':\n",
    "            labelweights = np.zeros(21)\n",
    "            for seg in self.semantic_labels_list:\n",
    "                tmp,_ = np.histogram(seg,range(22))\n",
    "                labelweights += tmp\n",
    "            labelweights = labelweights.astype(np.float32)\n",
    "            labelweights = labelweights/np.sum(labelweights)\n",
    "            self.labelweights = 1/np.log(1.2+labelweights)\n",
    "        elif split=='val':\n",
    "            self.labelweights = np.ones(21)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.with_rgb:\n",
    "            point_set_ini = self.scene_points_list[index]\n",
    "        else:\n",
    "            point_set_ini = self.scene_points_list[index][:, 0:3]\n",
    "        semantic_seg_ini = self.semantic_labels_list[index].astype(np.int32)\n",
    "        coordmax = np.max(point_set_ini[:, 0:3],axis=0)\n",
    "        coordmin = np.min(point_set_ini[:, 0:3],axis=0)\n",
    "        nsubvolume_x = np.ceil((coordmax[0]-coordmin[0])/1.5).astype(np.int32)\n",
    "        nsubvolume_y = np.ceil((coordmax[1]-coordmin[1])/1.5).astype(np.int32)\n",
    "        point_sets = list()\n",
    "        semantic_segs = list()\n",
    "        sample_weights = list()\n",
    "        for i in range(nsubvolume_x):\n",
    "            for j in range(nsubvolume_y):\n",
    "                curmin = coordmin+[i*1.5,j*1.5,0]\n",
    "                curmax = coordmin+[(i+1)*1.5,(j+1)*1.5,coordmax[2]-coordmin[2]]\n",
    "                curchoice = np.sum((point_set_ini[:, 0:3]>=(curmin-0.2))*(point_set_ini[:, 0:3]<=(curmax+0.2)),axis=1)==3\n",
    "                cur_point_set = point_set_ini[curchoice,0:3]\n",
    "                cur_point_full = point_set_ini[curchoice,:]\n",
    "                cur_semantic_seg = semantic_seg_ini[curchoice]\n",
    "                if len(cur_semantic_seg)==0:\n",
    "                    continue\n",
    "                mask = np.sum((cur_point_set>=(curmin-0.001))*(cur_point_set<=(curmax+0.001)),axis=1)==3\n",
    "                choice = np.random.choice(len(cur_semantic_seg), self.npoints, replace=True)\n",
    "                point_set = cur_point_full[choice,:] # Nx3/6\n",
    "                semantic_seg = cur_semantic_seg[choice] # N\n",
    "                mask = mask[choice]\n",
    "                if sum(mask)/float(len(mask))<0.01:\n",
    "                    continue\n",
    "                sample_weight = self.labelweights[semantic_seg]\n",
    "                sample_weight *= mask # N\n",
    "                point_sets.append(np.expand_dims(point_set,0)) # 1xNx3\n",
    "                semantic_segs.append(np.expand_dims(semantic_seg,0)) # 1xN\n",
    "                sample_weights.append(np.expand_dims(sample_weight,0)) # 1xN\n",
    "        point_sets = np.concatenate(tuple(point_sets),axis=0)\n",
    "        semantic_segs = np.concatenate(tuple(semantic_segs),axis=0)\n",
    "        sample_weights = np.concatenate(tuple(sample_weights),axis=0)\n",
    "        return point_sets, semantic_segs, sample_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = ScannetDataset(root, split='val')\n",
    "VAL_DATASET = ScannetDataset(root, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(VAL_DATASET, batch_size=2, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testDataLoader = DataLoader(VAL_DATASET, batch_size=2, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in tqdm(enumerate(train_data_loader), total=len(train_data_loader), smoothing=0.9, disable=True):\n",
    "        points, target,w = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_pointcloud(xyz, rgb=None):\n",
    "#     pcd = PointCloud()\n",
    "#     pcd.points = Vector3dVector(xyz) # XYZ points\n",
    "#     if rgb != None:\n",
    "#         pcd.colors = Vector3dVector(rgb/ 255.0)  #open3d requires colors (RGB) to be in range[0,1]\n",
    "#     draw_geometries([pcd])\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_point_cloud_with_r_group(batch_data):\n",
    "    \"\"\" Randomly perturb the point clouds by small rotations\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    G4 = np.array([[[1., 0., 0.],\n",
    "                     [0., 1., 0.],\n",
    "                     [0., 0., 1.]],\n",
    "\n",
    "                    [[1., 0., 0.],\n",
    "                     [0., 0., -1.],\n",
    "                     [0., 1., 0.]],\n",
    "\n",
    "                    [[1., 0., 0.],\n",
    "                     [0., -1., 0.],\n",
    "                     [0., 0., -1.]],\n",
    "\n",
    "                    [[1., 0., 0.],\n",
    "                     [0., 0., 1.],\n",
    "                     [0., -1., 0.]], ])\n",
    "    \n",
    "    G1_grouped_xyz = torch.mm(batch_data, G4[1])\n",
    "\n",
    "    return G1_grouped_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(tag, t):\n",
    "    print(\"{}: {}s\".format(tag, time() - t))\n",
    "    return time()\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx\n",
    "\n",
    "\n",
    "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        radius:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
    "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
    "    torch.cuda.empty_cache()\n",
    "    new_xyz = index_points(xyz, fps_idx)\n",
    "    torch.cuda.empty_cache()\n",
    "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "    torch.cuda.empty_cache()\n",
    "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
    "    torch.cuda.empty_cache()\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = index_points(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "    if returnfps:\n",
    "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
    "    else:\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "def sample_and_group_all(xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, 3]\n",
    "        new_points: sampled points data, [B, 1, N, 3+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    return new_xyz, new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance_conv(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "def index_points_conv(points, idx):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "def farthest_point_sample_conv(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, C]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    # Chien.dotruong 12-11-2020 Add\n",
    "    chien_centroids = torch.mean(xyz, dim=1)\n",
    "    chien_distance = torch.sum((xyz - chien_centroids.view(B, 1, C)) ** 2, dim=2)\n",
    "    farthest = torch.argmax(chien_distance.reshape(B,N),dim=1)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def query_ball_point_conv(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, C]\n",
    "        new_xyz: query points, [B, S, C]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance_conv(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx\n",
    "\n",
    "def knn_point_conv(nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, C]\n",
    "        new_xyz: query points, [B, S, C]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    sqrdists = square_distance_conv(new_xyz, xyz)\n",
    "    _, group_idx = torch.topk(sqrdists, nsample, dim = -1, largest=False, sorted=False)\n",
    "    return group_idx\n",
    "\n",
    "def sample_and_group_conv(npoint, nsample, xyz, points, density_scale = None):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, C]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, C]\n",
    "        new_points: sampled points data, [B, 1, N, C+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample_conv(xyz, npoint) # [B, npoint, C]\n",
    "    new_xyz = index_points_conv(xyz, fps_idx)\n",
    "    idx = knn_point_conv(nsample, xyz, new_xyz)\n",
    "    grouped_xyz = index_points_conv(xyz, idx) # [B, npoint, nsample, C]\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "    if points is not None:\n",
    "        grouped_points = index_points_conv(points, idx)\n",
    "        # print(grouped_points.shape)\n",
    "        # print(\"Debug grouped points: {}\".format(grouped_points[0,0,0,0:5]))\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "\n",
    "    if density_scale is None:\n",
    "        return new_xyz, new_points, grouped_xyz_norm, idx\n",
    "    else:\n",
    "        grouped_density = index_points_conv(density_scale, idx)\n",
    "        return new_xyz, new_points, grouped_xyz_norm, idx, grouped_density\n",
    "\n",
    "def sample_and_group_all_conv(xyz, points, density_scale = None):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, C]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, C]\n",
    "        new_points: sampled points data, [B, 1, N, C+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    #new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    new_xyz = xyz.mean(dim = 1, keepdim = True)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C) - new_xyz.view(B, 1, 1, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    if density_scale is None:\n",
    "        return new_xyz, new_points, grouped_xyz\n",
    "    else:\n",
    "        grouped_density = density_scale.view(B, 1, N, 1)\n",
    "        return new_xyz, new_points, grouped_xyz, grouped_density\n",
    "    \n",
    "\n",
    "def group_conv(nsample, xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, C]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, C]\n",
    "        new_points: sampled points data, [B, 1, N, C+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = N\n",
    "    new_xyz = xyz\n",
    "    idx = knn_point_conv(nsample, xyz, new_xyz)\n",
    "    grouped_xyz = index_points_conv(xyz, idx) # [B, npoint, nsample, C]\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "    if points is not None:\n",
    "        grouped_points = index_points_conv(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "\n",
    "    return new_points, grouped_xyz_norm\n",
    "\n",
    "# def compute_density(xyz, bandwidth):\n",
    "#     '''\n",
    "#     xyz: input points position data, [B, N, C]\n",
    "#     '''\n",
    "#     #import ipdb; ipdb.set_trace()\n",
    "#     B, N, C = xyz.shape\n",
    "#     sqrdists = square_distance_conv(xyz, xyz)\n",
    "#     gaussion_density = torch.exp(- sqrdists / (2.0 * bandwidth * bandwidth)) / (2.5 * bandwidth)\n",
    "#     xyz_density = gaussion_density.mean(dim = -1)\n",
    "\n",
    "#     return xyz_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_density(xyz, bandwidth):\n",
    "    '''\n",
    "    xyz: input points position data, [B, N, C]\n",
    "    '''\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    B, N, C = xyz.shape\n",
    "    sqrdists = square_distance(xyz, xyz)\n",
    "    gaussion_density = torch.exp(- sqrdists / (2.0 * bandwidth * bandwidth)) / (2.5 * bandwidth)\n",
    "    xyz_density = gaussion_density.mean(dim = -1)\n",
    "\n",
    "    return xyz_density\n",
    "\n",
    "class DensityNet(nn.Module):\n",
    "    def __init__(self, hidden_unit = [16, 8]):\n",
    "        super(DensityNet, self).__init__()\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList() \n",
    "\n",
    "        self.mlp_convs.append(nn.Conv2d(1, hidden_unit[0], 1))\n",
    "        self.mlp_bns.append(nn.BatchNorm2d(hidden_unit[0]))\n",
    "        for i in range(1, len(hidden_unit)):\n",
    "            self.mlp_convs.append(nn.Conv2d(hidden_unit[i - 1], hidden_unit[i], 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(hidden_unit[i]))\n",
    "        self.mlp_convs.append(nn.Conv2d(hidden_unit[-1], 1, 1))\n",
    "        self.mlp_bns.append(nn.BatchNorm2d(1))\n",
    "\n",
    "    def forward(self, density_scale):\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            density_scale =  bn(conv(density_scale))\n",
    "            if i == len(self.mlp_convs):\n",
    "                density_scale = F.sigmoid(density_scale)\n",
    "            else:\n",
    "                density_scale = F.relu(density_scale)\n",
    "        \n",
    "        return density_scale\n",
    "\n",
    "class WeightNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, hidden_unit = [8, 8]):\n",
    "        super(WeightNet, self).__init__()\n",
    "\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        if hidden_unit is None or len(hidden_unit) == 0:\n",
    "            self.mlp_convs.append(nn.Conv2d(in_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "        else:\n",
    "            self.mlp_convs.append(nn.Conv2d(in_channel, hidden_unit[0], 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(hidden_unit[0]))\n",
    "            for i in range(1, len(hidden_unit)):\n",
    "                self.mlp_convs.append(nn.Conv2d(hidden_unit[i - 1], hidden_unit[i], 1))\n",
    "                self.mlp_bns.append(nn.BatchNorm2d(hidden_unit[i]))\n",
    "            self.mlp_convs.append(nn.Conv2d(hidden_unit[-1], out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "        \n",
    "    def forward(self, localized_xyz):\n",
    "        #xyz : BxCxKxN\n",
    "\n",
    "        weights = localized_xyz\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            weights =  F.relu(bn(conv(weights)))\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    \n",
    "class PointNetSetAbstraction(nn.Module):\n",
    "#     npoint, nsample, in_channel, mlp, bandwidth, group_all\n",
    "    def __init__(self, npoint, nsample, in_channel, mlp, bandwidth,group_all):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "#         self.radius = radius\n",
    "        self.bandwidth = bandwidth\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "    \n",
    "\n",
    "        self.weightnet = WeightNet(3, 16)\n",
    "        self.linear = nn.Linear(16 * mlp[-1], mlp[-1])\n",
    "        self.bn_linear = nn.BatchNorm1d(mlp[-1])\n",
    "        self.densitynet = DensityNet()\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"SA\")\n",
    "        \n",
    "        B = xyz.shape[0]\n",
    "        N = xyz.shape[2]\n",
    "        \n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        xyz_density = compute_density(xyz, self.bandwidth)\n",
    "        inverse_density = 1.0 / xyz_density \n",
    "        \n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points, grouped_xyz_norm, grouped_density = sample_and_group_all_conv(xyz,\n",
    "                                                                                               points,\n",
    "                                                                                               inverse_density.view(B, N, 1))\n",
    "        else:\n",
    "            new_xyz, new_points, grouped_xyz_norm, _, grouped_density = sample_and_group_conv(self.npoint,\n",
    "                                                                                              self.nsample,\n",
    "                                                                                              xyz,\n",
    "                                                                                              points,\n",
    "                                                                                              inverse_density.view(B, N, 1))\n",
    "\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        print(\"----new_points_1:\", new_points.shape)\n",
    "        \n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "            \n",
    "        print(\"----new_points_none_linear_1:\", new_points.shape)\n",
    "            \n",
    "        inverse_max_density = grouped_density.max(dim = 2, keepdim=True)[0]\n",
    "        density_scale = grouped_density / inverse_max_density\n",
    "        density_scale = self.densitynet(density_scale.permute(0, 3, 2, 1))\n",
    "\n",
    "        new_points = new_points * density_scale\n",
    "\n",
    "        grouped_xyz = grouped_xyz_norm.permute(0, 3, 2, 1)\n",
    "        weights = self.weightnet(grouped_xyz)     \n",
    "        new_points = torch.matmul(input=new_points.permute(0, 3, 1, 2), other = weights.permute(0, 3, 2, 1)).view(B, self.npoint, -1)\n",
    "        \n",
    "        \n",
    "        new_points = self.linear(new_points)\n",
    "        print(\"----new_points_none_linear_2:\",new_points.shape)\n",
    "        new_points = self.bn_linear(new_points.permute(0, 2, 1))\n",
    "        new_points = F.relu(new_points)\n",
    "        \n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        #add new noneline\n",
    "        \n",
    "        print(\"----new_points_out:\",new_points.shape)\n",
    "\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, npoint, nsample, in_channel, out_put, mlp, bandwidth,group_all):\n",
    "        super(PointNetFeaturePropagation, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        \n",
    "        self.mlp_convs2d = nn.ModuleList()\n",
    "        self.mlp_bns2d = nn.ModuleList()\n",
    "        self.out_put = out_put\n",
    "        \n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs2d.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns2d.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "            \n",
    "        last_channel = out_put\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "            \n",
    "            \n",
    "        self.bandwidth = bandwidth\n",
    "        self.group_all = group_all\n",
    "        \n",
    "        self.weightnet = WeightNet(3, 16)\n",
    "        self.linear = nn.Linear(16 * mlp[-1], mlp[-1])\n",
    "        self.bn_linear = nn.BatchNorm1d(mlp[-1])\n",
    "        self.densitynet = DensityNet()\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            points1: input points data, [B, D, N]\n",
    "            points2: input points data, [B, D, S]\n",
    "        Return:\n",
    "            new_points: upsampled points data, [B, D', N]\n",
    "        \"\"\"\n",
    "        print(\"FP\")\n",
    "        xyz1 = xyz1.permute(0, 2, 1)\n",
    "        xyz2 = xyz2.permute(0, 2, 1)\n",
    "\n",
    "        points2 = points2.permute(0, 2, 1)\n",
    "        B, N, C = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated_points = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dists = square_distance(xyz1, xyz2)\n",
    "            dists, idx = dists.sort(dim=-1)\n",
    "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "\n",
    "            dist_recip = 1.0 / (dists + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
    "            \n",
    "        print(\"----interpolated_points:\", interpolated_points.shape)\n",
    "        \n",
    "        #deconv\n",
    "            \n",
    "        xyz_density = compute_density(xyz1, self.bandwidth)\n",
    "        inverse_density = 1.0 / xyz_density\n",
    "        \n",
    "        \n",
    "        if self.group_all:\n",
    "            new_xyz, new_points, grouped_xyz_norm, grouped_density = sample_and_group_all_conv(xyz1,\n",
    "                                                                                               interpolated_points,\n",
    "                                                                                               inverse_density.view(B, N, 1))\n",
    "        else:\n",
    "            new_xyz, new_points, grouped_xyz_norm, _, grouped_density = sample_and_group_conv(self.npoint,\n",
    "                                                                                              self.nsample,\n",
    "                                                                                              xyz1,\n",
    "                                                                                              interpolated_points,\n",
    "                                                                                              inverse_density.view(B, N, 1))\n",
    "        \n",
    "        \n",
    "        print(\"----new_points:\", new_points.shape)\n",
    "        \n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        print(\"----new_points_permute_1:\", new_points.shape)\n",
    "        \n",
    "        for i, conv in enumerate(self.mlp_convs2d):\n",
    "#             print(\"i:\",i)\n",
    "            bn = self.mlp_bns2d[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "#             print(\"-----new_points:\",new_points.shape)\n",
    "            \n",
    "        print(\"----new_points_none_linear:\", new_points.shape)\n",
    "        \n",
    "        \n",
    "        ########weight-net#############3\n",
    "        grouped_xyz = grouped_xyz_norm.permute(0, 3, 2, 1)\n",
    "        weights = self.weightnet(grouped_xyz) \n",
    "        print(\"----weights:\", weights.shape)\n",
    "        \n",
    "        ################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################\n",
    "        if points1 is not None:\n",
    "            print(\"points1:\", points1.shape)\n",
    "            points1 = points1.permute(0, 2, 1)\n",
    "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
    "        else:\n",
    "            new_points = interpolated_points\n",
    "\n",
    "        new_points = new_points.permute(0, 2, 1)\n",
    "        \n",
    "        print(\"new_points_permute_FP:\",new_points.shape)\n",
    "        new_points.unsqueeze_(2)\n",
    "        new_points = new_points.permute(0, 1, 2, 3)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = F.relu(bn(conv(new_points)))\n",
    "            \n",
    "        new_points = torch.squeeze(new_points,dim=2)\n",
    "        \n",
    "        print(\"new_points_FP:\",new_points.shape)\n",
    "        return new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in tqdm(enumerate(train_data_loader), total=len(train_data_loader), smoothing=0.9, disable=True):\n",
    "        points, target,w = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = points.transpose(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_points = points\n",
    "l0_xyz = points[:,:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa1 = PointNetSetAbstraction(1024, 32, 6 + 3, [32, 32, 64],0.2, False)\n",
    "sa2 = PointNetSetAbstraction(256, 32, 64 + 3, [64, 64, 128],0.2, False)\n",
    "sa3 = PointNetSetAbstraction(64, 32, 128 + 3, [128, 128, 256],0.2, False)\n",
    "sa4 = PointNetSetAbstraction(16, 32, 256 + 3, [256, 256, 512],0.2, False)\n",
    "fp4 = PointNetFeaturePropagation(512, 32, 515, 768, [512, 256], bandwidth =0.1,group_all=False)\n",
    "fp3 = PointNetFeaturePropagation(256, 32, 259, 384, [256, 256], bandwidth =0.2,group_all=False)\n",
    "fp2 = PointNetFeaturePropagation(256, 32, 259, 320, [256, 128], bandwidth =0.4,group_all=False)\n",
    "fp1 = PointNetFeaturePropagation(128, 32, 131, 128, [128, 128, 128], bandwidth =0.6,group_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA\n",
      "----new_points_1: torch.Size([2, 9, 32, 1024])\n",
      "----new_points_none_linear_1: torch.Size([2, 64, 32, 1024])\n",
      "----new_points_none_linear_2: torch.Size([2, 1024, 64])\n",
      "----new_points_out: torch.Size([2, 64, 1024])\n",
      "SA\n",
      "----new_points_1: torch.Size([2, 67, 32, 256])\n",
      "----new_points_none_linear_1: torch.Size([2, 128, 32, 256])\n",
      "----new_points_none_linear_2: torch.Size([2, 256, 128])\n",
      "----new_points_out: torch.Size([2, 128, 256])\n",
      "SA\n",
      "----new_points_1: torch.Size([2, 131, 32, 64])\n",
      "----new_points_none_linear_1: torch.Size([2, 256, 32, 64])\n",
      "----new_points_none_linear_2: torch.Size([2, 64, 256])\n",
      "----new_points_out: torch.Size([2, 256, 64])\n",
      "SA\n",
      "----new_points_1: torch.Size([2, 259, 32, 16])\n",
      "----new_points_none_linear_1: torch.Size([2, 512, 32, 16])\n",
      "----new_points_none_linear_2: torch.Size([2, 16, 512])\n",
      "----new_points_out: torch.Size([2, 512, 16])\n"
     ]
    }
   ],
   "source": [
    "l1_xyz, l1_points = sa1(l0_xyz, l0_points)\n",
    "l2_xyz, l2_points = sa2(l1_xyz, l1_points)\n",
    "l3_xyz, l3_points = sa3(l2_xyz, l2_points)\n",
    "l4_xyz, l4_points = sa4(l3_xyz, l3_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP\n",
      "----interpolated_points: torch.Size([2, 64, 512])\n",
      "----new_points: torch.Size([2, 512, 32, 515])\n",
      "----new_points_permute_1: torch.Size([2, 515, 32, 512])\n",
      "----new_points_none_linear: torch.Size([2, 256, 32, 512])\n",
      "----weights: torch.Size([2, 16, 32, 512])\n",
      "points1: torch.Size([2, 256, 64])\n",
      "new_points_permute_FP: torch.Size([2, 768, 64])\n",
      "new_points_FP: torch.Size([2, 256, 64])\n",
      "FP\n",
      "----interpolated_points: torch.Size([2, 256, 256])\n",
      "----new_points: torch.Size([2, 256, 32, 259])\n",
      "----new_points_permute_1: torch.Size([2, 259, 32, 256])\n",
      "----new_points_none_linear: torch.Size([2, 256, 32, 256])\n",
      "----weights: torch.Size([2, 16, 32, 256])\n",
      "points1: torch.Size([2, 128, 256])\n",
      "new_points_permute_FP: torch.Size([2, 384, 256])\n",
      "new_points_FP: torch.Size([2, 256, 256])\n",
      "FP\n",
      "----interpolated_points: torch.Size([2, 1024, 256])\n",
      "----new_points: torch.Size([2, 256, 32, 259])\n",
      "----new_points_permute_1: torch.Size([2, 259, 32, 256])\n",
      "----new_points_none_linear: torch.Size([2, 128, 32, 256])\n",
      "----weights: torch.Size([2, 16, 32, 256])\n",
      "points1: torch.Size([2, 64, 1024])\n",
      "new_points_permute_FP: torch.Size([2, 320, 1024])\n",
      "new_points_FP: torch.Size([2, 128, 1024])\n",
      "FP\n",
      "----interpolated_points: torch.Size([2, 2048, 128])\n",
      "----new_points: torch.Size([2, 128, 32, 131])\n",
      "----new_points_permute_1: torch.Size([2, 131, 32, 128])\n",
      "----new_points_none_linear: torch.Size([2, 128, 32, 128])\n",
      "----weights: torch.Size([2, 16, 32, 128])\n",
      "new_points_permute_FP: torch.Size([2, 128, 2048])\n",
      "new_points_FP: torch.Size([2, 128, 2048])\n"
     ]
    }
   ],
   "source": [
    "l3_points = fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n",
    "l2_points = fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
    "l1_points = fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
    "l0_points = fp1(l0_xyz, l1_xyz, None, l1_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 2048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=21\n",
    "NUM_CLASSES=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(128, 128, 1)\n",
    "bn1 = nn.BatchNorm1d(128)\n",
    "drop1 = nn.Dropout(0.5)\n",
    "conv2 = nn.Conv1d(128, num_classes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = drop1(F.relu(bn1(conv1(l0_points))))\n",
    "x = conv2(x)\n",
    "x = F.log_softmax(x, dim=1)\n",
    "x = x.permute(0, 2, 1)\n",
    "# return x, l4_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_pred=x\n",
    "trans_feat = l4_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(get_loss, self).__init__()\n",
    "    def forward(self, pred, target, trans_feat, weight):\n",
    "        total_loss = F.nll_loss(pred, target, weight=weight)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = get_loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_pred, trans_feat = classifier(points)\n",
    "seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
    "target = target.view(-1, 1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(seg_pred, target, trans_feat, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
    "        super(PointNet2, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"points:\",points.shape)\n",
    "        \n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
    "            \n",
    "        print(\"new_points_grouping:\",new_points.shape)\n",
    "        # new_xyz: sampled points position data, [B, npoint, C]\n",
    "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "\n",
    "        new_points = torch.max(new_points, 2)[0]\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        \n",
    "        print(\"new_points_out:\",new_points.shape)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNet2FP(nn.Module):\n",
    "    def __init__(self, in_channel, mlp):\n",
    "        super(PointNet2FP, self).__init__()\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            points1: input points data, [B, D, N]\n",
    "            points2: input points data, [B, D, S]\n",
    "        Return:\n",
    "            new_points: upsampled points data, [B, D', N]\n",
    "        \"\"\"\n",
    "        xyz1 = xyz1.permute(0, 2, 1)\n",
    "        xyz2 = xyz2.permute(0, 2, 1)\n",
    "\n",
    "        points2 = points2.permute(0, 2, 1)\n",
    "        B, N, C = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated_points = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dists = square_distance(xyz1, xyz2)\n",
    "            dists, idx = dists.sort(dim=-1)\n",
    "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "\n",
    "            dist_recip = 1.0 / (dists + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
    "\n",
    "        if points1 is not None:\n",
    "            points1 = points1.permute(0, 2, 1)\n",
    "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
    "        else:\n",
    "            new_points = interpolated_points\n",
    "            \n",
    "        print(\"new_points_FP:\",new_points.shape)\n",
    "\n",
    "        new_points = new_points.permute(0, 2, 1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = F.relu(bn(conv(new_points)))\n",
    "            \n",
    "        print(\"new_points_FP:\",new_points.shape)\n",
    "        return new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_points = points\n",
    "l0_xyz = points[:,:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa1 = PointNet2(1024, 0.1, 32, 6 + 3, [32, 32, 64], False)\n",
    "sa2 = PointNet2(256, 0.2, 32, 64 + 3, [64, 64, 128], False)\n",
    "sa3 = PointNet2(64, 0.4, 32, 128 + 3, [128, 128, 256], False)\n",
    "sa4 = PointNet2(16, 0.8, 32, 256 + 3, [256, 256, 512], False)\n",
    "fp4 = PointNet2FP(768, [256, 256])\n",
    "fp3 = PointNet2FP(384, [256, 256])\n",
    "fp2 = PointNet2FP(320, [256, 128])\n",
    "fp1 = PointNet2FP(128, [128, 128, 128])\n",
    "conv1 = nn.Conv1d(128, 128, 1)\n",
    "bn1 = nn.BatchNorm1d(128)\n",
    "drop1 = nn.Dropout(0.5)\n",
    "conv2 = nn.Conv1d(128, num_classes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_xyz, l1_points = sa1(l0_xyz, l0_points)\n",
    "l2_xyz, l2_points = sa2(l1_xyz, l1_points)\n",
    "l3_xyz, l3_points = sa3(l2_xyz, l2_points)\n",
    "l4_xyz, l4_points = sa4(l3_xyz, l3_points)\n",
    "\n",
    "l3_points = fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n",
    "l2_points = fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
    "l1_points = fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
    "l0_points = fp1(l0_xyz, l1_xyz, None, l1_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> x = torch.zeros(2, 1, 2, 1, 2)\n",
    ">>> x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> y = torch.squeeze(x,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
